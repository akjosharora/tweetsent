Cleaning the text
  To improve the accuracy of predictions, we must have a good numerical representation of the text or tweets in our case, several
  Cleaning the text can add from 1 to 2% to overall accuracy for any classifier.
  step are taken in order to achieve that:
    - Converting the text to lower case.
    - Replace words preceded by not, no or words ending with "n't: (don't, mustn't, won't ...) with their opposite using an English dictionary.
    - Removing unwanted text:
      The following will be removed using regular expressions:
        - User names.
        - URLs.
        - Tags.
        - Empty tweets(Twitter API returns the string "Not Available" if the tweet doesn't contain a text).
        - Numbers.
        - Special characters.
    - Tokenization:
      The text will be tokenized into words.
    - Stop words removal:
      All stop words must be removed with the exception of no and not, these two were actually removed in step 2.
    - Stemming:
      All collected tokens or words are reduced to their stem, stemming does not always return a correct word, but it always reduced derived words to a single origin.

    To increase performance, a dataset is cleaned and stemmed only once then stored on disk, the user can always choose to use the cleaned set or the original one.
Turning text into numerical values:
  In order to turn the stemmed text or the original raw text into numerical values, we must build our vocabulary then
  calculate each vocabulary feature occurrences, scikit made it easy buy providing us with multiple vectorizers, the user has
  the option to use a simple count vectorizer or a TFIDF vectorizer(https://www.quora.com/How-does-TfidfVectorizer-work-in-laymans-terms#).

  Vectorization may need a large amount of RAM, two factors can help reduce the memory footprint but sadly the accuracy of classifications,
  min_df and max_features represent the minimum number of occurrences needed for a given word to be considered in the vocabulary and the size of the vocabulary.
  The user has the option to change both parameters when training models.

Training classifiers:
  The user can select from a wide range of classifiers, some classifiers are not good for scaling and maximum of 2k is recommended for training, linear classifiers can handle
  large data and that results in good overall accuracy.
  After the model is trained and tested, it is serialized and saved to disk.

Live tweet classification:
  All trained models are loaded from disk, tweets for a given topic are extracted from twitter and classified using each of the classifiers, user can then analyse
  and improve the accuracy depending on the trained models results.

Design patterns and methodology :
  TDD:
    The purpose of TDD is to enforce developers to write testable code by writing unit tests prior to the implementation and later check if the implementation
    passes the tests, We followed this pattern and wrote unit tests for the cleaning, machine learning and twitter modules, we used pytest to write our unit tests
    due to its simplicity since it only uses simple python assert syntax to test different  cases.

  Single purpose:
    Keeping The functionality of classes and methods limited to a single purpose helps both isolating problems and scaling, it also makes the code more readable.
    In our implementation we tried to keep our code clean and our classes and methods limited to single purpose so it can better tested and altered in the future.

  PEP8:
    For better code readability, we followed PEP8 guidelines, we may have missed a few times, but in general our code was written in compliance with the PEP8 guidelines.

Web interface:
  We used django as a backend framework for our project, django is the number one web development framework for python and is used to host many large websites such as Instagram.
  Using django will allow for scaling in the future, and hosting a django website can be very easy using some robust services such as heroku.
